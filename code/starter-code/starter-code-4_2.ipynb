{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing Lab\n",
    "\n",
    "In this lab we will further explore Scikit's and NLTK's capabilities to process text. We will use the 20 Newsgroup dataset, which is provided by Scikit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handlers could be found for logger \"sklearn.datasets.twenty_newsgroups\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "categories = [\n",
    "    'alt.atheism',\n",
    "    'talk.religion.misc',\n",
    "    'comp.graphics',\n",
    "    'sci.space',\n",
    "]\n",
    "\n",
    "data_train = fetch_20newsgroups(subset='train', categories=categories,\n",
    "                                shuffle=True, random_state=42,\n",
    "                                remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "data_test = fetch_20newsgroups(subset='test', categories=categories,\n",
    "                               shuffle=True, random_state=42,\n",
    "                               remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data inspection\n",
    "\n",
    "We have downloaded a few newsgroup categories and removed headers, footers and quotes.\n",
    "\n",
    "Let's inspect them.\n",
    "\n",
    "1. What data taype is `data_train`\n",
    "> sklearn.datasets.base.Bunch\n",
    "- Is it like a list? Or like a Dictionary? or what?\n",
    "> Dict\n",
    "- How many data points does it contain?\n",
    "- Inspect the first data point, what does it look like?\n",
    "> A blurb of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2034"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_train.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bag of Words model\n",
    "\n",
    "Let's train a model using a simple count vectorizer\n",
    "\n",
    "1. Initialize a standard CountVectorizer and fit the training data\n",
    "- how big is the feature dictionary\n",
    "- repeat eliminating english stop words\n",
    "- is the dictionary smaller?\n",
    "- transform the training data using the trained vectorizer\n",
    "- what are the 20 words that are most common in the whole corpus?\n",
    "- what are the 20 most common words in each of the 4 classes?\n",
    "- evaluate the performance of a Lotistic Regression on the features extracted by the CountVectorizer\n",
    "    - you will have to transform the test_set too. Be carefule to use the trained vectorizer, without re-fitting it\n",
    "- try the following 3 modification:\n",
    "    - restrict the max_features\n",
    "    - change max_df and min_df\n",
    "    - use a fixed vocabulary of size 80 combining the 20 most common words per group found earlier\n",
    "- for each of the above print a confusion matrix and investigate what gets mixed\n",
    "> Anwer: not surprisingly if we reduce the feature space we lose accuracy\n",
    "- print out the number of features for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the dict is (2034, 26879)\n",
      "the dict is (2034, 26576) with stopwords\n",
      "It is barely smaller\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "v = CountVectorizer()\n",
    "x_vect = v.fit_transform(data_train.data)\n",
    "print \"the dict is %s\" % str(x_vect.shape)\n",
    "\n",
    "v = CountVectorizer(stop_words = 'english')\n",
    "x_vect = v.fit_transform(data_train.data)\n",
    "print \"the dict is %s with stopwords\" % str(x_vect.shape)\n",
    "print \"It is barely smaller\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'know': 10, u'don': 2, u'say': 14, u'just': 9, u'like': 11, u'edu': 3, u'people': 13, u'time': 17, u'data': 0, u'image': 7, u'jesus': 8, u'space': 15, u'graphics': 6, u'does': 1, u'think': 16, u'nasa': 12, u'good': 5, u'use': 18, u'god': 4, u'way': 19}\n"
     ]
    }
   ],
   "source": [
    "top_20 = CountVectorizer(stop_words = 'english', max_features = 20)\n",
    "top_20.fit_transform(data_train.data)\n",
    "print top_20.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 20 by class\n",
      "{0: {u'people': 12, u'time': 17, u'argument': 0, u'say': 15, u'religion': 13, u'atheists': 2, u'don': 6, u'jesus': 8, u'does': 5, u'way': 19, u'true': 18, u'atheism': 1, u'said': 14, u'just': 9, u'think': 16, u'bible': 4, u'like': 11, u'god': 7, u'believe': 3, u'know': 10}, 1: {u'format': 7, u'data': 2, u'image': 11, u'gif': 9, u'ftp': 8, u'graphics': 10, u'does': 3, u'software': 18, u'available': 0, u'use': 19, u'pub': 17, u'like': 15, u'images': 12, u'file': 5, u'edu': 4, u'jpeg': 13, u'color': 1, u'program': 16, u'files': 6, u'know': 14}, 2: {u'people': 12, u'time': 17, u'data': 0, u'just': 3, u'year': 19, u'space': 16, u'launch': 4, u'orbit': 11, u'new': 10, u'don': 1, u'lunar': 6, u'shuttle': 15, u'like': 5, u'earth': 2, u'satellite': 14, u'moon': 8, u'program': 13, u'mission': 7, u'nasa': 9, u'use': 18}, 3: {u'life': 11, u'people': 13, u'time': 18, u'say': 16, u'jesus': 8, u'does': 4, u'way': 19, u'think': 17, u'don': 5, u'said': 15, u'just': 9, u'did': 3, u'bible': 1, u'good': 7, u'believe': 0, u'point': 14, u'like': 12, u'god': 6, u'christian': 2, u'know': 10}}\n"
     ]
    }
   ],
   "source": [
    "X = pd.DataFrame(data_train.data, columns =['data'])\n",
    "y = pd.DataFrame(data_train.target, columns =['class'])\n",
    "stuff = pd.concat([X, y], axis = 1)\n",
    "temp = []\n",
    "top_20_byclass = {}\n",
    "\n",
    "vect = CountVectorizer(stop_words = 'english', max_features = 20)\n",
    "for i in range(4):\n",
    "    temp = stuff[stuff['class'] == i]['data']\n",
    "    vect.fit_transform(temp)\n",
    "    top_20_byclass[i] = vect.vocabulary_\n",
    "\n",
    "print \"top 20 by class\"\n",
    "print top_20_byclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit stuff\n",
      "   pred_0  pred_1  pred_2  pred_3\n",
      "0     187      16      46      70\n",
      "1      13     345      28       3\n",
      "2      22      23     333      16\n",
      "3      67      14      27     143\n",
      "acc score\n",
      "0.745011086475\n",
      "number of words\n",
      "26576\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "logit = LogisticRegression()\n",
    "\n",
    "def vectorizer(v):\n",
    "    X_vect = v.fit_transform(data_train.data)\n",
    "    y_train = data_train.target\n",
    "\n",
    "    X_test_vect = v.transform(data_test.data)\n",
    "    y_test = data_test.target\n",
    "\n",
    "    logit.fit(X_vect, y_train)\n",
    "    y_pred = logit.predict(X_test_vect)\n",
    "    cnf = confusion_matrix(y_test, y_pred)\n",
    "    labels =  ['pred_0','pred_1','pred_2','pred_3']\n",
    "    labels2 =  ['0','1','2','3']\n",
    "    print pd.DataFrame(cnf, columns =labels, index = labels2)\n",
    "    print \"acc score\"\n",
    "    print logit.score(X_test_vect,y_test)\n",
    "    print \"number of words\"\n",
    "    print len(v.vocabulary_)\n",
    "\n",
    "v = CountVectorizer(stop_words = 'english')\n",
    "print \"Logit stuff\"\n",
    "vectorizer(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc score Logit, only 100 features\n",
      "   pred_0  pred_1  pred_2  pred_3\n",
      "0     161      28      56      74\n",
      "1      19     302      59       9\n",
      "2      38      34     298      24\n",
      "3     107      19      48      77\n",
      "acc score\n",
      "0.619364375462\n",
      "number of words\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "v = CountVectorizer(stop_words = 'english', max_features = 100)\n",
    "print \"Acc score Logit, only 100 features\"\n",
    "vectorizer(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc score Logit, max_df = .002\n",
      "   pred_0  pred_1  pred_2  pred_3\n",
      "0     137     106      38      38\n",
      "1      11     341      32       5\n",
      "2      32     137     217       8\n",
      "3      56      91      32      72\n",
      "acc score\n",
      "0.566888396157\n",
      "number of words\n",
      "21260\n"
     ]
    }
   ],
   "source": [
    "v = CountVectorizer(stop_words = 'english', max_df = .002)\n",
    "print \"Acc score Logit, max_df = .002\"\n",
    "vectorizer(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc score Logit, min_df = .2\n",
      "   pred_0  pred_1  pred_2  pred_3\n",
      "0      42     205      70       2\n",
      "1      23     264     101       1\n",
      "2      31     257     105       1\n",
      "3      25     180      44       2\n",
      "acc score\n",
      "0.305247597931\n",
      "number of words\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "v = CountVectorizer(stop_words = 'english', min_df = .2)\n",
    "print \"Acc score Logit, min_df = .2\"\n",
    "vectorizer(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc score Logit with only 80\n",
      "   pred_0  pred_1  pred_2  pred_3\n",
      "0     160      67      33      59\n",
      "1      25     314      44       6\n",
      "2      37      88     247      22\n",
      "3      90      70      15      76\n",
      "acc score\n",
      "0.589061345159\n",
      "number of words\n",
      "54\n"
     ]
    }
   ],
   "source": [
    "fixed_80 = []\n",
    "for x in top_20_byclass:\n",
    "    fixed_80 += top_20_byclass[x]\n",
    "\n",
    "v = CountVectorizer(stop_words = 'english', vocabulary = list(set(fixed_80)))\n",
    "print \"Acc score Logit with only 80\"\n",
    "vectorizer(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hashing and TF-IDF\n",
    "\n",
    "Let's see if Hashing or TF-IDF improves the accuracy.\n",
    "\n",
    "1. Initialize a HashingVectorizer and repeat the test with no restriction on the number of features\n",
    "- does the score improve with respect to the count vectorizer?\n",
    "    - can you change any of the default parameters to improve it?\n",
    "- print out the number of features for this model\n",
    "- Initialize a TF-IDF Vectorizer and repeat the analysis above\n",
    "- can you improve on your best score above?\n",
    "    - can you change any of the default parameters to improve it?\n",
    "- print out the number of features for this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hashing\n",
      "   pred_0  pred_1  pred_2  pred_3\n",
      "0     197      15      65      42\n",
      "1       9     347      32       1\n",
      "2      21      23     350       0\n",
      "3      86      18      44     103\n",
      "acc score\n",
      "0.736881005174\n",
      "number of words\n",
      "1048576\n",
      "did not improve\n",
      "I don't see any parameters that would improve it\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer, TfidfVectorizer\n",
    "\n",
    "def func(v):\n",
    "    X_vect = v.fit_transform(data_train.data)\n",
    "    y_train = data_train.target\n",
    "\n",
    "    X_test_vect = v.transform(data_test.data)\n",
    "    y_test = data_test.target\n",
    "\n",
    "    logit.fit(X_vect, y_train)\n",
    "    y_pred = logit.predict(X_test_vect)\n",
    "    cnf = confusion_matrix(y_test, y_pred)\n",
    "    labels =  ['pred_0','pred_1','pred_2','pred_3']\n",
    "    labels2 =  ['0','1','2','3']\n",
    "    print pd.DataFrame(cnf, columns =labels, index = labels2)\n",
    "    \n",
    "    print \"acc score\"\n",
    "    print logit.score(X_test_vect,y_test)\n",
    "    print \"number of words\"\n",
    "    print v.n_features\n",
    "\n",
    "print \"Hashing\"\n",
    "hv = HashingVectorizer(stop_words = 'english')\n",
    "func(hv)\n",
    "print \"did not improve\"\n",
    "print \"I don't see any parameters that would improve it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF\n",
      "   pred_0  pred_1  pred_2  pred_3\n",
      "0     198      15      65      41\n",
      "1       8     351      29       1\n",
      "2      17      21     356       0\n",
      "3      82      16      46     107\n",
      "acc score\n",
      "0.747967479675\n",
      "number of words\n",
      "26576\n",
      "it is .002 better than above\n"
     ]
    }
   ],
   "source": [
    "print \"TFIDF\"\n",
    "tv = TfidfVectorizer(stop_words = 'english')\n",
    "vectorizer(tv)\n",
    "print \"it is .002 better than above\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF\n",
      "   pred_0  pred_1  pred_2  pred_3\n",
      "0     194      14      66      45\n",
      "1       7     351      30       1\n",
      "2      13      22     359       0\n",
      "3      79      16      46     110\n",
      "acc score\n",
      "0.749445676275\n",
      "number of words\n",
      "26572\n",
      "changing max_df to .2 improves score\n"
     ]
    }
   ],
   "source": [
    "print \"TFIDF\"\n",
    "tv = TfidfVectorizer(stop_words = 'english', max_df = .2)\n",
    "vectorizer(tv)\n",
    "print \"changing max_df to .2 improves score\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Classifier comparison\n",
    "\n",
    "Of all the vectorizers tested above, choose one that has a reasonable performance with a manageable number of features and compare the performance of these models:\n",
    "\n",
    "- KNN\n",
    "- Logistic Regression\n",
    "- Decision Trees\n",
    "- Support Vector Machine\n",
    "- Random Forest\n",
    "- Extra Trees\n",
    "\n",
    "In order to speed up the calculation it's better to vectorize the data only once and then compare the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "#logit\n",
    "#hashing \n",
    "\n",
    "v = TfidfVectorizer(stop_words = 'english')\n",
    "X_vect = v.fit_transform(data_train.data)\n",
    "y_train = data_train.target\n",
    "\n",
    "X_test_vect = v.transform(data_test.data)\n",
    "y_test = data_test.target\n",
    "    \n",
    "def vectorizer2(model):\n",
    "    model.fit(X_vect, y_train)\n",
    "    y_pred = model.predict(X_test_vect)\n",
    "    cnf = confusion_matrix(y_test, y_pred)\n",
    "    labels =  ['pred_0','pred_1','pred_2','pred_3']\n",
    "    labels2 =  ['0','1','2','3']\n",
    "    print pd.DataFrame(cnf, columns =labels, index = labels2)\n",
    "    print \"acc score\"\n",
    "    print model.score(X_test_vect,y_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "   pred_0  pred_1  pred_2  pred_3\n",
      "0     198      15      65      41\n",
      "1       8     351      29       1\n",
      "2      17      21     356       0\n",
      "3      82      16      46     107\n",
      "acc score\n",
      "0.747967479675\n"
     ]
    }
   ],
   "source": [
    "print \"Logistic Regression\"\n",
    "model = LogisticRegression()\n",
    "vectorizer2(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier\n",
      "   pred_0  pred_1  pred_2  pred_3\n",
      "0     146      55      42      76\n",
      "1      21     315      36      17\n",
      "2      33      75     261      25\n",
      "3      84      53      16      98\n",
      "acc score\n",
      "0.606060606061\n"
     ]
    }
   ],
   "source": [
    "print \"DecisionTreeClassifier\"\n",
    "model = DecisionTreeClassifier()\n",
    "vectorizer2(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier\n",
      "   pred_0  pred_1  pred_2  pred_3\n",
      "0     196      33      35      55\n",
      "1      25     325      32       7\n",
      "2      44      63     275      12\n",
      "3     127      31      20      73\n",
      "acc score\n",
      "0.642276422764\n"
     ]
    }
   ],
   "source": [
    "print \"RandomForestClassifier\"\n",
    "model = RandomForestClassifier()\n",
    "vectorizer2(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier\n",
      "   pred_0  pred_1  pred_2  pred_3\n",
      "0     195      34      45      45\n",
      "1      21     345      20       3\n",
      "2      33      63     293       5\n",
      "3      92      34      40      85\n",
      "acc score\n",
      "0.678492239468\n"
     ]
    }
   ],
   "source": [
    "print \"ExtraTreesClassifier\"\n",
    "model = ExtraTreesClassifier()\n",
    "vectorizer2(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier\n",
      "   pred_0  pred_1  pred_2  pred_3\n",
      "0     115      95      55      54\n",
      "1     140     117      64      68\n",
      "2     111     137      80      66\n",
      "3      66      87      42      56\n",
      "acc score\n",
      "0.271988174427\n"
     ]
    }
   ],
   "source": [
    "print \"KNeighborsClassifier\"\n",
    "model = KNeighborsClassifier()\n",
    "vectorizer2(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC\n",
      "   pred_0  pred_1  pred_2  pred_3\n",
      "0       0       0     319       0\n",
      "1       0       0     389       0\n",
      "2       0       0     394       0\n",
      "3       0       0     251       0\n",
      "acc score\n",
      "0.291204730229\n"
     ]
    }
   ],
   "source": [
    "print \"SVC\"\n",
    "model = SVC()\n",
    "vectorizer2(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Other classifiers\n",
    "\n",
    "Adapt the code from [this example](http://scikit-learn.org/stable/auto_examples/text/document_classification_20newsgroups.html#example-text-document-classification-20newsgroups-py) to compare across all the classifiers suggested and to display the final plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: NLTK\n",
    "\n",
    "NLTK is a vast library. Can you find some interesting bits to share with classmates?\n",
    "Start here: http://www.nltk.org/"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
